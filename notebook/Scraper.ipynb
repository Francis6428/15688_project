{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "\n",
    "The main task of this part is to scrap the basic information of alumnis. It will use the [API](https://www.mediawiki.org/wiki/API:Main_page) provided by wikipedia to accelerate the scraping process. We will only scrap the introduction part of wiki page.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We just need `json` and `requests` libraries in this part. The only new library you need to install is [tqdm](https://pypi.python.org/pypi/tqdm). It is an easy-to-use progress meter for the python script. Since the number of alumnis are relatively large, tqdm could let us know the exact progress and estimated running time. You can install it using `pip`:\n",
    "\n",
    "    $ pip install --upgrade tqdm\n",
    "    \n",
    "After you finished the installation, please make sure the following commands are workable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories\n",
    "The main resource of alumni list is the [category](https://en.wikipedia.org/wiki/Help:Category) page of wikipedia. For example, the list of [Carnegie Mellon University alumni](https://en.wikipedia.org/wiki/Category:Carnegie_Mellon_University_alumni). Belowing is a dictionary with unviersities' name as keys and with the category name as values. We could manually add any unviersities we want into the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "universities = dict()\n",
    "universities['CMU'] = \"Category:Carnegie_Mellon_University_alumni\"\n",
    "universities['Stanford'] = \"Category:Stanford_University_alumni\"\n",
    "universities['Harvard'] = \"Category:Harvard_University_alumni\"\n",
    "universities['Yale'] = \"Category:Yale_University_alumni\"\n",
    "universities['UCLA'] = \"Category:University_of_California,_Los_Angeles_alumni\"\n",
    "universities['MIT'] = \"Category:Massachusetts_Institute_of_Technology_alumni\"\n",
    "universities['Pitt'] = \"Category:University_of_Pittsburgh_alumni\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Tittles Recursively\n",
    "\n",
    "Using [API:Categorymembers](https://www.mediawiki.org/wiki/API:Categorymembers), we could collect all the titles of alumni in a category page. With parameter `format=json`, we could get the json format as following (`cmlimit=2`):\n",
    "```python\n",
    "{\n",
    "    \"batchcomplete\": \"\",\n",
    "    \"continue\": {\n",
    "        \"cmcontinue\": \"page|29374306043f51394d0453454303063f51394d0453454304293743011e01dcc2dcc2dcc3dcbedc06|5648968\",\n",
    "        \"continue\": \"-||\"\n",
    "    },\n",
    "    \"query\": {\n",
    "        \"categorymembers\": [\n",
    "            {\n",
    "                \"pageid\": 14941891,\n",
    "                \"ns\": 0,\n",
    "                \"title\": \"Gregory Abowd\"\n",
    "            },\n",
    "            {\n",
    "                \"pageid\": 18446374,\n",
    "                \"ns\": 0,\n",
    "                \"title\": \"Linda Addison (poet)\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "Since the maximum number of titles we could get from one request is 500, we need to use the `continue` parameter. If there is still any remaining members after a request, there would be a `continue` field in the return json. By passing the value of `continue` filed to the `continue` parameter of next request, we could grab all the members recursively. Function `get_members` does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_members(cat_name, cont=None):\n",
    "    \"\"\"\n",
    "    Return the list of members in given category\n",
    "\n",
    "    Args:\n",
    "        cat_name (string): name of category in form as 'Category:Carnegie_Mellon_University_alumni'\n",
    "        cont (string): value of continue parameter, users should not use it\n",
    "    Returns:\n",
    "        ret_val (list): list of all titles of members in this page only\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\"action\" : 'query', 'format' : 'json', 'list' : 'categorymembers', 'cmlimit' : 500, 'cmprop' : 'title'}\n",
    "    params['cmtitle'] = cat_name\n",
    "    if (cont != None):\n",
    "        params['cmcontinue'] = cont\n",
    "    response = requests.get(url, params=params)\n",
    "    ret_val = []\n",
    "    data = json.loads(response.text)\n",
    "    if ('continue' in data):\n",
    "        ret_val = get_members(cat_name, cont=data['continue']['cmcontinue'])\n",
    "    ret_val += [x['title'] for x in data['query']['categorymembers']]\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Subcategories Recursively\n",
    "\n",
    "Notice that there are also some subcategories. Using same API, we could grap all subcategories similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_subcategories(cat_name, cont=None):\n",
    "    \"\"\"\n",
    "    Return the list of all subcategories\n",
    "\n",
    "    Args:\n",
    "        cat_name (string): name of category in form as 'Category:Carnegie_Mellon_University_alumni'\n",
    "        cont (string): value of continue parameter, users should not use it\n",
    "    Returns:\n",
    "        ret_val (list): list of all subcategories\n",
    "    \"\"\"\n",
    "        \n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\"action\" : 'query', 'format' : 'json', 'list' : 'categorymembers', 'cmlimit' : 500, 'cmtype' : 'subcat'}\n",
    "    params['cmtitle'] = cat_name\n",
    "    if (cont != None):\n",
    "        params['cmcontinue'] = cont\n",
    "    response = requests.get(url, params=params)\n",
    "    ret_val = []\n",
    "    data = json.loads(response.text)\n",
    "    if ('continue' in data):\n",
    "        ret_val = get_all_subcategories(cat_name, cont=data['continue']['cmcontinue'])\n",
    "    ret_val += [x['title'] for x in data['query']['categorymembers']]\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect All Tittles\n",
    "\n",
    "Combine the above two functions, we could grab all tittles in a category page, incluing the tittles in the subcategory pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_members(cat_name):\n",
    "    \"\"\"\n",
    "    Return the list of all members\n",
    "\n",
    "    Args:\n",
    "        cat_name (string): name of category in form as 'Category:Carnegie_Mellon_University_alumni'\n",
    "    Returns:\n",
    "        ret_val (list): list of all members\n",
    "    \"\"\"\n",
    "    subcategories = get_all_subcategories(cat_name)\n",
    "    ret_val = []\n",
    "    for subcat in subcategories:\n",
    "        ret_val += get_members(subcat)\n",
    "    ret_val += get_members(cat_name)\n",
    "    return list(filter(lambda a: not a.startswith(\"Category:\"), ret_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Introduction From a Wiki Page\n",
    "\n",
    "Now we have a list of tittles contained in a category page, the next step is to grab introduction of every wiki page. We could do that using [API: query/prop=extracts](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bextracts). With the request \n",
    "``` python\n",
    "'https://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&exintro=&explaintext=&titles=Gregory_Abowd|Ross%20Cohen'\n",
    "```\n",
    "the returned json is like:\n",
    "```python\n",
    "{\n",
    "    \"batchcomplete\": \"\",\n",
    "    \"query\": {\n",
    "        \"normalized\": [\n",
    "            {\n",
    "                \"from\": \"Gregory_Abowd\",\n",
    "                \"to\": \"Gregory Abowd\"\n",
    "            }\n",
    "        ],\n",
    "        \"pages\": {\n",
    "            \"14941891\": {\n",
    "                \"pageid\": 14941891,\n",
    "                \"ns\": 0,\n",
    "                \"title\": \"Gregory Abowd\",\n",
    "                \"extract\": \"Gregory Dominic Abowd is a computer scientist best known for his work in ubiquitous computing, software engineering, and technologies for autism. He is the J.Z. Liang Professor in the School of Interactive Computing at the Georgia Institute of Technology, where he joined the faculty in 1994.\"\n",
    "            },\n",
    "            \"34107971\": {\n",
    "                \"pageid\": 34107971,\n",
    "                \"ns\": 0,\n",
    "                \"title\": \"Ross Cohen\",\n",
    "                \"extract\": \"Ross Cohen is the cofounder of BitTorrent Inc along with brother Bram Cohen, where among other things he was involved in the Codeville project. He attended Carnegie Mellon University and Stuyvesant High School. He was forced out of the company in 2006.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "Notice that we could connect titles with `|` to get serveral request done at same time. It will greatly improve the efficientcy. The following function `get_info` will do this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_info(titles):\n",
    "    \"\"\"\n",
    "    Return a dictionary of {title : intro}\n",
    "    \n",
    "    Args:\n",
    "        titles (list of string): list of all titles\n",
    "    Returns:\n",
    "        ret_val: a dictionary of {title : intro}\n",
    "    \"\"\"\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\"action\" : 'query', 'format' : 'json', 'prop' : 'extracts', 'exintro' : '', 'explaintext' : ''}\n",
    "    params['titles'] = titles\n",
    "    response = requests.get(url, params=params)\n",
    "    data = list(json.loads(response.text)['query']['pages'].values())\n",
    "    ret_val = dict()\n",
    "    for d in data:\n",
    "        if (d['extract'] != \"\"):\n",
    "            ret_val[d['title']] = d['extract']\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect All Data\n",
    "\n",
    "Finally, we would combine all functions into a single function that grap all alumni's information. The data is formated as a json:\n",
    "```python\n",
    "{\n",
    "    'CMU' : {\n",
    "        'Kathleen_Carley' : \"Kathleen M. Carley is an American social scientist ...\"\"\n",
    "        ...\n",
    "    }\n",
    "    ...\n",
    "    'University_Name':{\n",
    "        'Person_Name' : \"Text\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_info(title_list):\n",
    "    \"\"\"\n",
    "    Return a dictionary of {title : intro}. This function would connect tittles into \n",
    "    one request less than 200 characters to improve efficiency.\n",
    "    \n",
    "    Args:\n",
    "        titles (list of string): list of all titles\n",
    "    Returns:\n",
    "        ret_val: a dictionary of {title : intro}\n",
    "    \"\"\"\n",
    "    data = dict()\n",
    "    partial_titles = \"\"\n",
    "    for title in tqdm(title_list):\n",
    "        if len(partial_titles + title) < 200:\n",
    "            partial_titles += title + '|'\n",
    "        else:\n",
    "            partial_titles += title\n",
    "            info = get_info(partial_titles)\n",
    "            data = {**data, **info}\n",
    "            partial_titles = \"\"\n",
    "    return data\n",
    "def get_all_data(universities, append=False):\n",
    "    \"\"\"\n",
    "    dump a data dictionary into data.json file\n",
    "    \n",
    "    Args:\n",
    "        universities(dict) : {university name : alumni list category name}\n",
    "        append(bool) :  if append is true, the data would be added into existing data dictionary\n",
    "                        if append is false, the old data would be replaced\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    data = dict()\n",
    "    if (append):\n",
    "        with open('data.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "    for key, value in universities.items():\n",
    "        print(key)\n",
    "        data[key] = get_all_info(get_all_members(value))\n",
    "    with open('data.json', 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:09<00:00, 88.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3870/3870 [00:46<00:00, 82.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19706/19706 [04:11<00:00, 78.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7414/7414 [01:36<00:00, 76.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2727/2727 [00:31<00:00, 86.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:36<00:00, 81.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 796/796 [00:09<00:00, 84.16it/s]\n"
     ]
    }
   ],
   "source": [
    "get_all_data(universities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrap Academic Disciplines\n",
    "\n",
    "We also need some \"Ground Truth\" for different [academic disciplines](https://en.wikipedia.org/wiki/Outline_of_academic_disciplines) in later analysis. We need to import `BeautifulSoup` for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the html of this page, we found that all `h3` tags are the discipline names we want and the link to this discipline is very close to `h3` tag. Thus we could use `BeautifulSoup` to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arts': ['/wiki/The_arts'], 'History': ['/wiki/History'], 'Languages and literature': ['/wiki/Language', '/wiki/Literature'], 'Philosophy': ['/wiki/Philosophy'], 'Theology': ['/wiki/Theology'], 'Anthropology': ['/wiki/Anthropology'], 'Economics': ['/wiki/Economics'], 'Human geography': ['/wiki/Human_geography'], 'Law': ['/wiki/Law'], 'Political science': ['/wiki/Politics', '/wiki/Political_science'], 'Psychology': ['/wiki/Psychology', '/wiki/List_of_psychology_disciplines'], 'Sociology': ['/wiki/Sociology'], 'Biology': ['/wiki/List_of_life_sciences'], 'Chemistry': ['/wiki/Chemistry'], 'Earth sciences': ['/wiki/Earth_science'], 'Space sciences': [], 'Physics': ['/wiki/Physics'], 'Computer Science': ['/wiki/Computer_science'], 'Mathematics': ['/wiki/Mathematics'], 'Statistics': ['/wiki/Statistics'], 'Engineering and technology': ['/wiki/Engineering'], 'Medicine and health': ['/wiki/Medicine', '/wiki/Healthcare_science']}\n"
     ]
    }
   ],
   "source": [
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return (response.status_code, response.text)\n",
    "    pass\n",
    "#Scrap Academic Disciplines\n",
    "outline_url = \"https://en.wikipedia.org/wiki/Outline_of_academic_disciplines\"\n",
    "code, html = retrieve_html(outline_url)\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "body = soup.find('div',{'id':'bodyContent'})\n",
    "disciplines = dict()\n",
    "for h3 in body.find_all('h3'):\n",
    "    dis_name = h3.text[:-6]\n",
    "    note = h3.next_sibling.next_sibling\n",
    "    disciplines[dis_name] = []\n",
    "    if (dis_name == 'Human geography'):\n",
    "        disciplines[dis_name].append('/wiki/Human_geography')\n",
    "        continue\n",
    "    for link in note.find_all('a', href=True):\n",
    "        url = link.attrs['href']\n",
    "        if (url.find('Outline')==-1):\n",
    "            disciplines[dis_name].append(url)\n",
    "print(disciplines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `disciplines.json` is formatted as following:\n",
    "```python\n",
    "{\n",
    "    'Discipline Name' : ['Doc1', 'Doc2', ...]\n",
    "    'Arts' : ['The arts refers to the theory and physical expression of creativity found in human societies and cultures...']\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
