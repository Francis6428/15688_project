{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "import string\n",
    "import scipy\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    \"\"\"\n",
    "    This function takes a a string of text. Then tokenize it.\n",
    "    \n",
    "    Args:\n",
    "        text: a text string\n",
    "    Returns:\n",
    "        a list of tokens(words)\n",
    "    \"\"\"\n",
    "    stpwords = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    proc = ''\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        c = text[i]\n",
    "        #remove punctuation\n",
    "        if c not in string.punctuation:\n",
    "            proc+=c\n",
    "        elif c == \"'\":\n",
    "            if i == (len(text)-1):\n",
    "                break\n",
    "            if text[i+1] == 's' and (i+2) == len(text):\n",
    "                break\n",
    "            if text[i+1] == 's' and text[i+2] == ' ':\n",
    "                i+=1\n",
    "        else:\n",
    "            proc+=' '\n",
    "        i+=1  \n",
    "    #tokenization by nltk\n",
    "    tokens = nltk.word_tokenize(proc)\n",
    "    #remove stop words in tokens\n",
    "    result = []\n",
    "    for word in tokens:\n",
    "        if word not in stpwords:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_data = {}\n",
    "for school in data.keys():\n",
    "    tokenized_data[school] = {}\n",
    "    for person in data[school].keys():\n",
    "        tokenized_data[school][person] = tokenization(data[school][person])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenized_data.json', 'w') as fp:\n",
    "    json.dump(tokenized_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load google word2vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding(tokens, model):\n",
    "    \"\"\"\n",
    "    This function takes a list of tokens(words), and a pretrained word2vec model in gensim.\n",
    "        Then transform the tokens into a single vector.\n",
    "    Args: \n",
    "        tokens: a list of string representing the tokens\n",
    "        model: a pretrained word2vec gensim model\n",
    "    returns:\n",
    "        vec: a np array representing the tokens.\n",
    "    \"\"\"\n",
    "    vec = None\n",
    "    for word in tokens:\n",
    "        #if the model does not contain the word. just skip it\n",
    "        if word not in model.vocab:\n",
    "            continue\n",
    "        if type(vec) != type(None):\n",
    "            vec = vec + model[word]\n",
    "        else:\n",
    "            vec = model[word]\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for school in tokenized_data.keys():\n",
    "    processed_data[school] = {}\n",
    "    for person in data[school].keys():\n",
    "        processed_data[school][person] = embedding(tokenized_data[school][person],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Den%C3%A9e_Benton', 'Kathleen_Carley', 'Greg_Mottola', 'Steven_Bochco', 'Nik_Bonaddio', 'Keith_Lockhart', 'Roberta_Klatzky', 'Megan_Hilty', 'Jeffrey_Mylett', 'Sada_Thompson', 'Lars_Peter_Hansen', 'James_Cromwell', 'John_Robert_Anderson_(psychologist)', 'William_A._Barnett', 'Gabriel_Macht', 'Tamara_Tunie', 'Allan_Meltzer', 'Lori_Rom', 'Jeffrey_Zaslow', 'Frank_Converse', 'Gaius_Charles', 'Merton_Miller', 'David_Hornsby', 'Alan_Perlis', 'Guy_L._Steele_Jr.', 'Renee_Elise_Goldsberry', 'Lourdes_Benedicto', 'Harry_Shum', 'John_Pasquin', 'Clifford_Shull', 'John_Wells_(TV_producer)', 'Jack_Klugman', 'John_Currin', 'Otto_Stern', 'Henry_Mazer', 'Lou_Scheimer', 'Robert_V._Rice', 'Drew_D._Perkins', 'Rhys_Coiro', 'Charles_Erwin_Wilson', 'Paul_Lauterbur', 'Leslie_Valiant', 'Edward_C._Prescott', 'Herb_Gardner', 'David_Haskell', 'Arthur_Lubin', 'Ivan_Sutherland', 'Joe_Manganiello', 'Sydney_Kamlager', 'Daniel_Nagin', 'Pradeep_Sindhu', 'Richard_Duffin', 'Franco_Modigliani', 'Teresa_Heinz', 'Kai-Fu_Lee', 'John_Patrick_Crecine', 'Rich_Lackner', 'Ann_Roth', 'Charles_E._Leiserson', 'Jules_Fisher', 'Andrew_Ng', 'Neil_Druckmann', 'Andrew_W._Mellon', 'Joseph_F_Traub', 'Sheridan_Titman', 'William_Ball_(director)', 'Michelle_Veintimilla', 'James_Meena', 'Golan_Levin', 'Sam_Hyde', 'Robert_Lucas_Jr', 'Van_Hansis', 'Nancy_Marchand', 'Sutton_Foster', 'Frank_L._Stulen', 'Patricia_Tallman', 'Dan_Friedman_(graphic_designer)', 'George_Pake', 'Astro_Teller', 'Randy_Pausch', 'Anita_K._Jones', 'James_Gosling', 'Charles_Haid', 'Herb_Sendek', 'Stephen_Fienberg', 'Joyce_Kozloff', 'Paul_Mellon', 'Ravi_Jagannathan', 'Bhakta_B._Rath', 'Edward_Creutz', 'Charles_L._Evans', 'William_Atherton', 'Abby_Brammell', 'Kara_Lindsay', 'William_Wulf', 'Manuel_Blum', 'Oliver_Williamson', 'George_Cowan', 'John_Forbes_Nash', 'David_Larsen', 'Red_Whittaker', 'Pablo_Schreiber', 'Kushagra_Bajaj', 'Clarence_Zener', 'H._John_Heinz_III', 'Virgil_D._Gligor', 'Charles_Geschke', 'Dagmara_Dominczyk', 'David_M._Kelley', 'Shanghua_Teng', 'Stephen_Schwartz_(composer)', 'Casey_Cott', 'Michael_Tucker_(actor)', 'Judith_Light', 'Jay_Nunamaker', 'Robert_Griffiths_(physicist)', 'Andy_Warhol', 'George_Loewenstein', 'Michael_C._McFarland', 'H._T._Kung', 'Jean_Carson', 'Billy_Porter_(entertainer)', 'David_Tepper', 'Holly_Hunter', 'Stephanie_Kwolek', 'George_A._Romero', 'Francisco_D%27Souza', 'George_Peppard', 'Jonathan_Borofsky', 'Raoul_Bott', 'Matthew_Bomer', 'Edward_Feigenbaum', 'Caren_Kaye', 'Zachary_Quinto', 'James_McClelland_(psychologist)', 'Paul_Flory', 'Anna_Deavere_Smith', 'Norman_Ren%C3%A9', 'Merton_H._Miller', 'Cherry_Jones', 'Bill_Peduto', 'Lorrie_Cranor', 'Donna_Lynne_Champlin', 'Robert_Dennard', 'Alexander_Knaster', 'Albert_Brooks', 'Andy_Bechtolsheim', 'Bud_Yorkin', 'Josef_Sommer', 'Sian_Heder', 'Nathaniel_Borenstein', 'Dana_S._Scott', 'John_Pople', 'Philip_Pearlstein', 'Finn_E._Kydland', 'Paula_Wagner', 'Richard_Wallace_(scientist)', 'Mariette_Hartley', 'John_Muth', 'Robert_Floyd', 'Fern_Persons', 'Brent_Barrett', 'Qi_Lu_(computer_scientist)', 'Christina_Crawford', 'Robert_Foxworth', 'Nicole_DeHuff', 'Yoshiaki_Fujimori', 'Farnam_Jahanian', 'David_Lander', 'Cote_de_Pablo', 'Javier_Soltero', 'John_L._Hall', 'Katy_Mixon', 'Sulajja_Firodia_Motwani', 'Luis_von_Ahn', 'Egon_Balas', 'William_Larimer_Mellon,_Sr.', 'Ada_Yonath', 'Joshua_Bloch', 'Dwight_Dike_Beede', 'Raymond_Kaskey', 'Natalie_Venetia_Belcon', 'Shafi_Goldwasser', 'Brian_MacWhinney', 'Gerald_C._Meyers', 'Frank_Gorshin', 'Geoffrey_Hinton', 'Stephanie_Palmer', 'Romesh_Wadhwani', 'Sally_Jessy_Rapha%C3%ABl', 'Jared_Cohon', 'Josh_Groban', 'Oliver_E._Williamson', 'Richard_Rashid', 'Arthur_Kennedy_(actor)', 'Anthony_Daniels', 'Yoky_Matsuoka', 'Shari_Belafonte', 'James_Goodby', 'Derek_Stephen_Prince', 'Raj_Reddy', 'Sheila_Butler', 'Jerry_D._Thompson', 'James_G._March', 'Peter_Corroon', 'Laura_San_Giacomo', 'Guyford_Stever', 'Krzysztof_Matyjaszewski', 'Ming-Na_Wen', 'Michael_McMillian', 'Daniel_Sleator', 'Sonia_Manzano', 'John_Graham_(policy_analyst)', 'Harrison_White', 'Elizabeth_Carpenter', 'Richard_Rappaport', 'Philip_Morrison', 'Frederick_Mosteller', 'Michael_Keaton', 'Aron_Ralston', 'Henry_Hornbostel', 'Mel_Bochner', 'Bob_Altemeyer', 'Watts_Humphrey', 'Vasili_Kuznetsov_(politician)', 'Dale_Thomas_Mortensen', 'Gilmer_McCormick', 'James_Jacks', 'John_L._Anderson', 'Ren%C3%A9_Auberjonois', 'Brian_Berry', 'Elizabeth_Bailey', 'Dana_Scott', 'Dina_Dublon', 'Demetrius_Grosse', 'Ellen_Crawford', 'Barbara_Feldon', 'Walter_Kohn', 'Ted_Danson', 'Michael_McKean', 'David_Farber', 'Robert_Schmertz_(artist)', 'Nada_Arkaji', 'William_S._Dietrich_II', 'Ian_Harding', 'Jimmy_Robertson_(American_football)', 'Ethan_Hawke', 'Paul_Ben-Victor', 'David_Norona', 'Carmen_Yul%C3%ADn_Cruz', 'Scott_Fahlman', 'Cormac_Kinney', 'Corey_Cott', 'Josh_Gad', 'James_D._Meindl', 'Alfred_Spector', 'Bob_Cummings', 'Patina_Miller', 'Walter_Dill_Scott', 'Francois_Clemmons', 'Patrick_Wilson_(American_actor)', 'Michael_Goldenberg', 'Telly_Leung', 'Shafrira_Goldwasser', 'Subra_Suresh', 'Rob_Marshall', 'John_McDaniel_(musician)', 'Joe_Rudolph', 'Lenore_Blum', 'Robert_Lucas,_Jr.', 'David_Parnas', 'Lori_Cardille', 'Kim_Director', 'Yarone_Zober', 'John_Ousterhout', 'Mark_Russinovich', 'Sidney_Furie', 'Rory_O%27Malley', 'Virgil_Cantini', 'Jonathan_I._Schwartz', 'Erin_Mackey', 'Andrew_Carnegie', 'Aaron_Staton', 'Edmund_M._Clarke', 'Richard_Cyert', 'C._D._Mote,_Jr.', 'Gust_Avrakotos', 'Jeffrey_Pfeffer', 'Rich_Fitzgerald', 'Frederick_Koehler', 'Randal_Bryant', 'Kali_Rocha', 'Blair_Underwood', 'Herbert_A._Simon', 'Christian_Borle', 'James_G._Mitchell', 'Robert_Lepper', 'Leslie_Odom,_Jr.', 'Allen_Newell'])\n"
     ]
    }
   ],
   "source": [
    "print(processed_data['CMU'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('vec_data.npz', **processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.load('vec_data.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CMU', 'UCLA', 'Stanford', 'Harvard', 'Berklee']\n"
     ]
    }
   ],
   "source": [
    "print(test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
